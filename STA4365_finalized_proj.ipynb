{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a170038",
      "metadata": {
        "id": "4a170038"
      },
      "source": [
        "## Project\n",
        "### Human vs ChatGPT\n",
        "#### Satyar Foroughi,  April 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20af25c8",
      "metadata": {
        "id": "20af25c8"
      },
      "outputs": [],
      "source": [
        "# In order to run this notebook in google collab\n",
        "# you need to run the following commands\n",
        "\n",
        "# !pip install datasets\n",
        "\n",
        "# !pip install torchtext==0.6.0  # newer versions of torchtext changed some libraries and\n",
        "                                 # I couldn't figure out how to use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75452043",
      "metadata": {
        "id": "75452043"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2160041",
      "metadata": {
        "id": "c2160041"
      },
      "outputs": [],
      "source": [
        "# Basic RNN which was used\n",
        "class RNN(torch.nn.Module):\n",
        "    '''\n",
        "    Basic RNN (LSTM) with 1 hidden layer.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
        "                                 hidden_dim)        \n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, answer):\n",
        "        embedded = self.embedding(answer)\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden.squeeze_(0)\n",
        "        output = self.fc(hidden)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42e6c687",
      "metadata": {
        "id": "42e6c687"
      },
      "outputs": [],
      "source": [
        "# A bit more complicated RNN, which was not used due to GPU limits\n",
        "# Performance is not known due to the limit\n",
        "class RNN_2(torch.nn.Module):\n",
        "    '''\n",
        "    RNN (LSTM) with 2 bidirectional hidden layers with dropout rate\n",
        "    '''\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
        "                                 hidden_dim,\n",
        "                                 num_layers=n_layers,\n",
        "                                 bidirectional=True,\n",
        "                                 dropout=dropout_rate if n_layers > 1 else 0)\n",
        "        self.fc = torch.nn.Linear(hidden_dim * 2, output_dim) \n",
        "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, answer):\n",
        "        embedded = self.dropout(self.embedding(answer))\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)        \n",
        "        output = self.fc(self.dropout(hidden))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6bc539",
      "metadata": {
        "id": "8e6bc539"
      },
      "outputs": [],
      "source": [
        "def count_special_chars(df, replace=False):\n",
        "    \"\"\"\n",
        "    Counts the number of rows in a DataFrame that contain special characters in the 'answer' or 'question' columns.\n",
        "    If replace=True, replaces special characters. Returns the numbers respectively.\n",
        "    \"\"\"\n",
        "    \n",
        "    if (replace==True):\n",
        "        # replace special characters in 'answer' column\n",
        "        df['answer'] = df['answer'].str.replace('\\n', ' ')\n",
        "        df['answer'] = df['answer'].str.replace('\\t', ' ')\n",
        "        df['answer'] = df['answer'].str.replace('\\r', '')\n",
        "        df['answer'] = df['answer'].str.replace('\\u202f', ' ')\n",
        "        \n",
        "        # replace special characters in 'question' column\n",
        "        df['question'] = df['question'].str.replace('\\n', ' ')\n",
        "        df['question'] = df['question'].str.replace('\\t', ' ')\n",
        "        df['question'] = df['question'].str.replace('\\r', '')\n",
        "        df['question'] = df['question'].str.replace('\\u202f', ' ')\n",
        "        \n",
        "                # new-line\n",
        "    mask_hum = ((df['answer'].str.contains('\\n'))\n",
        "                # tab\n",
        "                | (df['answer'].str.contains('\\t'))\n",
        "                # carriage return\n",
        "                | (df['answer'].str.contains('\\r'))\n",
        "                # narrow no-break space\n",
        "                | (df['answer'].str.contains('\\u202f'))) & (df['target'] == 0)\n",
        "\n",
        "    mask_gpt = ((df['answer'].str.contains('\\n'))\n",
        "                | (df['answer'].str.contains('\\t'))\n",
        "                | (df['answer'].str.contains('\\r'))\n",
        "                | (df['answer'].str.contains('\\u202f'))) & (df['target'] == 1)\n",
        "\n",
        "    mask_ques = ((df['question'].str.contains('\\n'))\n",
        "                 | (df['question'].str.contains('\\t'))\n",
        "                 | (df['question'].str.contains('\\r'))\n",
        "                 | (df['question'].str.contains('\\u202f')))\n",
        "    \n",
        "\n",
        "    print(\"human answers with special characters:\", len(df[mask_hum]))\n",
        "    print(\"chatgpt answers with special characters:\", len(df[mask_gpt]))\n",
        "    print(\"questions with special characters:\", len(df[mask_ques]))\n",
        "    \n",
        "    return len(df[mask_hum]), len(df[mask_gpt]), len(df[mask_ques])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbad0b06",
      "metadata": {
        "id": "dbad0b06"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data_loader, device):\n",
        "    '''\n",
        "    Compute the accuracy of the given model, with the given data.\n",
        "    '''\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_accuracy, num_batches = 0, 0\n",
        "        correct_pred, num_examples = 0, 0\n",
        "\n",
        "        for _, batch in enumerate(data_loader):\n",
        "\n",
        "            answer, _ = batch.answer\n",
        "            target = batch.target.to(device)\n",
        "\n",
        "            odds = model(answer)\n",
        "            probs = torch.sigmoid(odds)\n",
        "            predicted_labels = (probs > 0.5).float()\n",
        "\n",
        "            # Convert tensors to NumPy arrays for easier operations\n",
        "            predicted_labels_np = predicted_labels.cpu().numpy()\n",
        "            target_np = target.cpu().numpy()\n",
        "            \n",
        "            # Calculate accuracy for the current batch\n",
        "            corrects = [1 for i, pred in enumerate(predicted_labels_np) if (target_np[i]==pred)]\n",
        "            correct_pred = sum(corrects)\n",
        "            num_examples = target_np.shape[0]\n",
        "\n",
        "            batch_accuracy = (correct_pred / num_examples) * 100\n",
        "\n",
        "            total_accuracy += batch_accuracy\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_accuracy / num_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a442f195",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "a442f195",
        "outputId": "80630081-7210-417a-8515-b414f0f7aac1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nclass RNN(torch.nn.Module):\\n\\n    def __init__(self, input_dim_q, input_dim_a, embedding_dim, hidden_dim, output_dim):\\n        super().__init__()\\n\\n        self.embedding_question = torch.nn.Embedding(input_dim_q, embedding_dim)\\n        self.embedding_answer = torch.nn.Embedding(input_dim_a, embedding_dim)\\n        self.rnn = torch.nn.LSTM(embedding_dim,\\n                                 hidden_dim)        \\n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\\n\\n    def forward(self, question, question_length, answer, answer_length):        \\n        embedded_question = self.embedding_question(question)        \\n        embedded_answer = self.embedding_answer(answer)        \\n        embedded = torch.cat((embedded_question, embedded_answer), dim=0)        \\n        sorted_lengths, sorted_idx = torch.sort(question_length + answer_length, descending=True)\\n        embedded = embedded[:, sorted_idx]        \\n        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, sorted_lengths.to(\\'cpu\\'))\\n        packed_output, (hidden, cell) = self.rnn(packed)\\n        hidden.squeeze_(0)        \\n        _, original_idx = torch.sort(sorted_idx)\\n        output = self.fc(hidden[original_idx])\\n        return output\\n        \\ndef get_accuracy(model, data_loader, device):\\n\\n    with torch.no_grad():\\n        total_accuracy, num_batches = 0, 0\\n        correct_pred, num_examples = 0, 0\\n\\n\\n        for _, batch in enumerate(data_loader):\\n\\n            question, question_length = batch.question\\n            answer, answer_length = batch.answer\\n            target = batch.target.to(device)\\n\\n            odds = model(question, question_length, answer, answer_length)\\n            probs = torch.sigmoid(odds)\\n            predicted_labels = (probs > 0.5).float()\\n\\n            predicted_labels_np = predicted_labels.cpu().numpy()\\n            target_np = target.cpu().numpy()\\n\\n            corrects = [1 for i, pred in enumerate(predicted_labels_np) if (target_np[i]==pred)]\\n            correct_pred = sum(corrects)\\n            num_examples = target_np.shape[0]\\n\\n            batch_accuracy = (correct_pred / num_examples) * 100\\n\\n            total_accuracy += batch_accuracy\\n            num_batches += 1\\n\\n    return total_accuracy / num_batches\\n    \\n    \\ninput_dim_q = len(question_field.vocab)\\ninput_dim_a = len(answer_field.vocab)\\n\\nmodel = RNN(input_dim_q=input_dim_q,\\n            input_dim_a=input_dim_a,\\n            embedding_dim=params[\\'emb_dim\\'],\\n            hidden_dim=params[\\'hid_dim\\'],\\n            output_dim=params[\\'out_dim\\'] # could use 1 for binary classification\\n)\\n\\n\\n\\nmodel = model.to(DEVICE)\\noptimizer = torch.optim.Adam(model.parameters(), lr=params[\\'lr\\'])\\n\\nstart_time = time.time()\\nn_ep = params[\\'epochs\\']\\n\\nfor epoch in range(params[\\'epochs\\']):\\n    model.train()\\n    for batch_id, batch_data in enumerate(train_loader):\\n        \\n        question, q_length = batch_data.question\\n        answer, a_length = batch_data.answer\\n        target = batch_data.target.to(DEVICE)\\n\\n        odds = model(question, q_length, answer, a_length)\\n        probs = torch.sigmoid(odds)\\n        predicted_labels = (probs > 0.5).float()\\n        loss = nn.BCEWithLogitsLoss()(odds, target.unsqueeze(1).to(odds.dtype))\\n        optimizer.zero_grad()\\n        \\n        loss.backward()\\n        \\n        optimizer.step()\\n        \\n        if not batch_id % 50:\\n            print (f\\'Epoch: {epoch+1:03d}/{n_ep:03d} | \\'\\n                   f\\'Batch {batch_id:03d}/{len(train_loader):03d} | \\'\\n                   f\\'Loss: {loss:.4f}\\')\\n            \\n            predicted_labels_np = predicted_labels.cpu().numpy()\\n            target_np = target.cpu().numpy()\\n\\n            corrects = [1 for i, pred in enumerate(predicted_labels_np) if (target_np[i]==pred)]\\n            correct_pred = sum(corrects)\\n            num_examples = target_np.shape[0]\\n\\n            print(f\"{correct_pred} out of {num_examples}\")\\n            print(\"-----------------------------------------------------------\")\\n\\n    with torch.set_grad_enabled(False):\\n        print(f\\'training accuracy: \\'\\n              f\\'{get_accuracy(model, train_loader, DEVICE):.2f}%\\')\\n        \\n    print(f\\'Time elapsed: {(time.time() - start_time)/60:.2f} min\\')\\n    \\nprint(f\\'Total Training Time: {(time.time() - start_time)/60:.2f} min\\')\\nprint(f\\'Test accuracy: {get_accuracy(model, test_loader, DEVICE):.2f}%\\')\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# These following lines of code are if you want to use a model that takes both question and answer as inputs\n",
        "# As mentioned in the final report and presentation, this approach didn't yield any promising results\n",
        "# However, the format of the main parts of the code would be different. These are commented out\n",
        "'''\n",
        "class RNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim_q, input_dim_a, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_question = torch.nn.Embedding(input_dim_q, embedding_dim)\n",
        "        self.embedding_answer = torch.nn.Embedding(input_dim_a, embedding_dim)\n",
        "        self.rnn = torch.nn.LSTM(embedding_dim,\n",
        "                                 hidden_dim)        \n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, question, question_length, answer, answer_length):        \n",
        "        embedded_question = self.embedding_question(question)        \n",
        "        embedded_answer = self.embedding_answer(answer)        \n",
        "        embedded = torch.cat((embedded_question, embedded_answer), dim=0)        \n",
        "        sorted_lengths, sorted_idx = torch.sort(question_length + answer_length, descending=True)\n",
        "        embedded = embedded[:, sorted_idx]        \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, sorted_lengths.to('cpu'))\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        hidden.squeeze_(0)        \n",
        "        _, original_idx = torch.sort(sorted_idx)\n",
        "        output = self.fc(hidden[original_idx])\n",
        "        return output\n",
        "        \n",
        "def get_accuracy(model, data_loader, device):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_accuracy, num_batches = 0, 0\n",
        "        correct_pred, num_examples = 0, 0\n",
        "\n",
        "\n",
        "        for _, batch in enumerate(data_loader):\n",
        "\n",
        "            question, question_length = batch.question\n",
        "            answer, answer_length = batch.answer\n",
        "            target = batch.target.to(device)\n",
        "\n",
        "            odds = model(question, question_length, answer, answer_length)\n",
        "            probs = torch.sigmoid(odds)\n",
        "            predicted_labels = (probs > 0.5).float()\n",
        "\n",
        "            predicted_labels_np = predicted_labels.cpu().numpy()\n",
        "            target_np = target.cpu().numpy()\n",
        "\n",
        "            corrects = [1 for i, pred in enumerate(predicted_labels_np) if (target_np[i]==pred)]\n",
        "            correct_pred = sum(corrects)\n",
        "            num_examples = target_np.shape[0]\n",
        "\n",
        "            batch_accuracy = (correct_pred / num_examples) * 100\n",
        "\n",
        "            total_accuracy += batch_accuracy\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_accuracy / num_batches\n",
        "    \n",
        "    \n",
        "input_dim_q = len(question_field.vocab)\n",
        "input_dim_a = len(answer_field.vocab)\n",
        "\n",
        "model = RNN(input_dim_q=input_dim_q,\n",
        "            input_dim_a=input_dim_a,\n",
        "            embedding_dim=params['emb_dim'],\n",
        "            hidden_dim=params['hid_dim'],\n",
        "            output_dim=params['out_dim'] # could use 1 for binary classification\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\n",
        "start_time = time.time()\n",
        "n_ep = params['epochs']\n",
        "\n",
        "for epoch in range(params['epochs']):\n",
        "    model.train()\n",
        "    for batch_id, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        question, q_length = batch_data.question\n",
        "        answer, a_length = batch_data.answer\n",
        "        target = batch_data.target.to(DEVICE)\n",
        "\n",
        "        odds = model(question, q_length, answer, a_length)\n",
        "        probs = torch.sigmoid(odds)\n",
        "        predicted_labels = (probs > 0.5).float()\n",
        "        loss = nn.BCEWithLogitsLoss()(odds, target.unsqueeze(1).to(odds.dtype))\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        if not batch_id % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{n_ep:03d} | '\n",
        "                   f'Batch {batch_id:03d}/{len(train_loader):03d} | '\n",
        "                   f'Loss: {loss:.4f}')\n",
        "            \n",
        "            predicted_labels_np = predicted_labels.cpu().numpy()\n",
        "            target_np = target.cpu().numpy()\n",
        "\n",
        "            corrects = [1 for i, pred in enumerate(predicted_labels_np) if (target_np[i]==pred)]\n",
        "            correct_pred = sum(corrects)\n",
        "            num_examples = target_np.shape[0]\n",
        "\n",
        "            print(f\"{correct_pred} out of {num_examples}\")\n",
        "            print(\"-----------------------------------------------------------\")\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{get_accuracy(model, train_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {get_accuracy(model, test_loader, DEVICE):.2f}%')\n",
        "'''\n",
        "\n",
        "# Note that on top of replacing the desired blocks with the above,\n",
        "# some lines which are mentioned throughout the notebook (labeled \"for approach 1\")\n",
        "# need to be uncommented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a0a1804",
      "metadata": {
        "id": "7a0a1804"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "params = {\n",
        "    'emb_dim': 256,\n",
        "    'hid_dim': 256,\n",
        "    'out_dim': 1,\n",
        "    'n_layers': 2,\n",
        "    'dropout': 0.35,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 13,\n",
        "    'batch_size': 128,\n",
        "    'test_prc': 0.2,\n",
        "    'num_filters': 100,          # For a CNN which I decided to not include\n",
        "    'filter_sizes': [2, 3, 4],   # For a CNN which I decided to not include\n",
        "}\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15be4537",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15be4537",
        "outputId": "ea255718-2d4f-45c1-b043-6f49814ac129"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7732dec91a887272/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
          ]
        }
      ],
      "source": [
        "# Full dataset with list type columns in human and chatGPT answers\n",
        "dataset = load_dataset(\"json\", data_files=\"all.jsonl\", split=\"train\")\n",
        "df_full = dataset.to_pandas()\n",
        "\n",
        "data = []\n",
        "\n",
        "# Combining the human and GPT columns into one column \"answer\" which is type string\n",
        "# Making sure each question has the same ammount of human and chatGPT answers\n",
        "for i in range(len(df_full)):\n",
        "    question = df_full.loc[i, 'question']\n",
        "    j_min = min(len(df_full['human_answers'][i]), len(df_full['chatgpt_answers'][i]))\n",
        "    \n",
        "    for j in range(j_min):\n",
        "        h_ans = df_full.loc[i, 'human_answers'][j]\n",
        "        c_ans = df_full.loc[i, 'chatgpt_answers'][j]\n",
        "        \n",
        "        data.append({'question': question, 'answer': h_ans, 'target': 0})\n",
        "        data.append({'question': question, 'answer': c_ans, 'target': 1})\n",
        "\n",
        "# Making new dataset\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Dropping duplicates\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7689cca5",
      "metadata": {
        "id": "7689cca5"
      },
      "source": [
        "It seems like many chatgpt answers have special characters, but most human answers don't."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d0178a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09d0178a",
        "outputId": "c8f5fded-207d-450b-84a3-c02a1e9e5353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "human answers with special characters: 650\n",
            "chatgpt answers with special characters: 7645\n",
            "questions with special characters: 0\n"
          ]
        }
      ],
      "source": [
        "human_answers, chatgpt_answers, _ = count_special_chars(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e4d44e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "29e4d44e",
        "outputId": "6dbb1b88-e327-4a06-b8e6-a35c5227e183"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plotting to visualize the difference\n",
        "labels = ['Human Answers', 'ChatGPT Answers']\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "ax.bar(labels, [human_answers, chatgpt_answers])\n",
        "\n",
        "ax.set_title('Special Characters')\n",
        "\n",
        "ax.set_xlabel('Answers')\n",
        "ax.set_ylabel('Frequency')\n",
        "\n",
        "ax.text(0, human_answers, human_answers, ha='center', va='bottom')\n",
        "ax.text(1, chatgpt_answers, chatgpt_answers, ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afab1e4a",
      "metadata": {
        "id": "afab1e4a"
      },
      "source": [
        "This could be due to the way the data was collected and the different sources.<br>\n",
        "Having this significance of a difference could result in noise and bias, <br>\n",
        "Therefore it could be a good idea to replace these special characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d120056d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d120056d",
        "outputId": "0004650d-6473-4968-c1c6-c691bc863418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "human answers with special characters: 0\n",
            "chatgpt answers with special characters: 0\n",
            "questions with special characters: 0\n"
          ]
        }
      ],
      "source": [
        "human_answers, chatgpt_answers, _ = count_special_chars(df, replace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26c42d0",
      "metadata": {
        "id": "c26c42d0"
      },
      "outputs": [],
      "source": [
        "# shuffling the dataset\n",
        "shuffled_df = df.sample(frac=1, random_state=13)\n",
        "\n",
        "# For approach 1:\n",
        "    # save shuffled df as a csv file and use that instead of sh_2.csv\n",
        "    \n",
        "# For normal approach:\n",
        "    # Drop the question column\n",
        "sh_2 = shuffled_df.drop('question', axis=1)\n",
        "\n",
        "    # Save as csv\n",
        "# sh_2.to_csv('sh_2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "831e23f6",
      "metadata": {
        "id": "831e23f6"
      },
      "outputs": [],
      "source": [
        "# question_field = torchtext.data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm', include_lengths=True) # Approach 1\n",
        "answer_field = torchtext.data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm', include_lengths=True)\n",
        "label_field = torchtext.data.LabelField(dtype=torch.long)\n",
        "\n",
        "# fields = [('question', question_field), ('answer', answer_field), ('target', label_field)] # Approach 1\n",
        "fields = [('answer', answer_field), ('target', label_field)]\n",
        "\n",
        "dataset = torchtext.data.TabularDataset(path='sh_2.csv', format='csv', skip_header=True, fields=fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f526893",
      "metadata": {
        "id": "3f526893"
      },
      "outputs": [],
      "source": [
        "# Splitting dataset to training and testing\n",
        "train_data, test_data = dataset.split(split_ratio=[1-params['test_prc'], params['test_prc']],random_state=random.seed(13))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78dac9d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78dac9d9",
        "outputId": "3aa73c4a-ff25-4fa4-c420-a32a352716f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 83952\n",
            "Number of classes: 2\n"
          ]
        }
      ],
      "source": [
        "# question_field.build_vocab(train_data)  # Approach 1\n",
        "answer_field.build_vocab(train_data)\n",
        "label_field.build_vocab(train_data)\n",
        "\n",
        "# print(f'Vocabulary size: {len(question_field.vocab)}')  # Approach 1\n",
        "print(f'Vocabulary size: {len(answer_field.vocab)}')\n",
        "print(f'Number of classes: {len(label_field.vocab)}')\n",
        "\n",
        "# Note that the way the label field worked, it mapped 0s to 1s and 1s to 0s\n",
        "# which means it's the opposite of the target column (0s are chatGPT, 1s are human)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b648665",
      "metadata": {
        "id": "9b648665"
      },
      "outputs": [],
      "source": [
        "train_loader, test_loader = torchtext.data.BucketIterator.splits(\n",
        "        (train_data, test_data), \n",
        "        batch_size=params['batch_size'],\n",
        "        sort_within_batch=True,\n",
        "        sort_key=lambda x: len(x.answer),\n",
        "        device=DEVICE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9151a8b0",
      "metadata": {
        "id": "9151a8b0"
      },
      "outputs": [],
      "source": [
        "input_dim = len(answer_field.vocab)\n",
        "\n",
        "# Basic RNN\n",
        "model = RNN(input_dim=input_dim,\n",
        "            embedding_dim=params['emb_dim'],\n",
        "            hidden_dim=params['hid_dim'],\n",
        "            output_dim=params['out_dim']\n",
        ")\n",
        "\n",
        "# 2 layer RNN\n",
        "#model = RNN_2(input_dim=input_dim,\n",
        "#            embedding_dim=params['emb_dim'],\n",
        "#            hidden_dim=params['hid_dim'],\n",
        "#            output_dim=params['out_dim'],\n",
        "#            n_layers=params['n_layers'],\n",
        "#            dropout_rate=params['dropout']\n",
        "#)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f3ed75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32f3ed75",
        "outputId": "93988353-a69f-4188-ec5d-90c153267df4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001/013 | Batch 000/285 | Loss: 0.7209\n",
            "31 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 001/013 | Batch 050/285 | Loss: 0.8517\n",
            "5 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 001/013 | Batch 100/285 | Loss: 0.7492\n",
            "44 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 001/013 | Batch 150/285 | Loss: 0.5666\n",
            "97 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 001/013 | Batch 200/285 | Loss: 0.4061\n",
            "106 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 001/013 | Batch 250/285 | Loss: 0.2442\n",
            "115 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 94.75%\n",
            "Time elapsed: 0.20 min\n",
            "Epoch: 002/013 | Batch 000/285 | Loss: 0.1626\n",
            "121 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 002/013 | Batch 050/285 | Loss: 0.0798\n",
            "123 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 002/013 | Batch 100/285 | Loss: 0.1492\n",
            "122 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 002/013 | Batch 150/285 | Loss: 0.1822\n",
            "120 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 002/013 | Batch 200/285 | Loss: 0.1357\n",
            "122 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 002/013 | Batch 250/285 | Loss: 0.1133\n",
            "122 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 98.01%\n",
            "Time elapsed: 0.40 min\n",
            "Epoch: 003/013 | Batch 000/285 | Loss: 0.0323\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 003/013 | Batch 050/285 | Loss: 0.1289\n",
            "121 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 003/013 | Batch 100/285 | Loss: 0.0376\n",
            "125 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 003/013 | Batch 150/285 | Loss: 0.1068\n",
            "121 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 003/013 | Batch 200/285 | Loss: 0.0544\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 003/013 | Batch 250/285 | Loss: 0.0599\n",
            "125 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 98.77%\n",
            "Time elapsed: 0.60 min\n",
            "Epoch: 004/013 | Batch 000/285 | Loss: 0.0200\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 004/013 | Batch 050/285 | Loss: 0.0484\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 004/013 | Batch 100/285 | Loss: 0.0279\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 004/013 | Batch 150/285 | Loss: 0.0154\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 004/013 | Batch 200/285 | Loss: 0.0157\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 004/013 | Batch 250/285 | Loss: 0.0375\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.26%\n",
            "Time elapsed: 0.81 min\n",
            "Epoch: 005/013 | Batch 000/285 | Loss: 0.0163\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 005/013 | Batch 050/285 | Loss: 0.0927\n",
            "125 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 005/013 | Batch 100/285 | Loss: 0.0424\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 005/013 | Batch 150/285 | Loss: 0.0391\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 005/013 | Batch 200/285 | Loss: 0.1505\n",
            "122 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 005/013 | Batch 250/285 | Loss: 0.0878\n",
            "124 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.36%\n",
            "Time elapsed: 1.01 min\n",
            "Epoch: 006/013 | Batch 000/285 | Loss: 0.0129\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 006/013 | Batch 050/285 | Loss: 0.0289\n",
            "125 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 006/013 | Batch 100/285 | Loss: 0.0387\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 006/013 | Batch 150/285 | Loss: 0.0372\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 006/013 | Batch 200/285 | Loss: 0.0142\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 006/013 | Batch 250/285 | Loss: 0.0080\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.48%\n",
            "Time elapsed: 1.21 min\n",
            "Epoch: 007/013 | Batch 000/285 | Loss: 0.0106\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 007/013 | Batch 050/285 | Loss: 0.0047\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 007/013 | Batch 100/285 | Loss: 0.0077\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 007/013 | Batch 150/285 | Loss: 0.0078\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 007/013 | Batch 200/285 | Loss: 0.0459\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 007/013 | Batch 250/285 | Loss: 0.0211\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.69%\n",
            "Time elapsed: 1.41 min\n",
            "Epoch: 008/013 | Batch 000/285 | Loss: 0.0280\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 008/013 | Batch 050/285 | Loss: 0.0222\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 008/013 | Batch 100/285 | Loss: 0.0078\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 008/013 | Batch 150/285 | Loss: 0.0012\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 008/013 | Batch 200/285 | Loss: 0.0320\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 008/013 | Batch 250/285 | Loss: 0.0089\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.53%\n",
            "Time elapsed: 1.62 min\n",
            "Epoch: 009/013 | Batch 000/285 | Loss: 0.0413\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 009/013 | Batch 050/285 | Loss: 0.0419\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 009/013 | Batch 100/285 | Loss: 0.0095\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 009/013 | Batch 150/285 | Loss: 0.0640\n",
            "125 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 009/013 | Batch 200/285 | Loss: 0.0033\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 009/013 | Batch 250/285 | Loss: 0.0015\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.73%\n",
            "Time elapsed: 1.82 min\n",
            "Epoch: 010/013 | Batch 000/285 | Loss: 0.0022\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 010/013 | Batch 050/285 | Loss: 0.0007\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 010/013 | Batch 100/285 | Loss: 0.0013\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 010/013 | Batch 150/285 | Loss: 0.0387\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 010/013 | Batch 200/285 | Loss: 0.0206\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 010/013 | Batch 250/285 | Loss: 0.0025\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.78%\n",
            "Time elapsed: 2.02 min\n",
            "Epoch: 011/013 | Batch 000/285 | Loss: 0.0067\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 011/013 | Batch 050/285 | Loss: 0.0017\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 011/013 | Batch 100/285 | Loss: 0.0023\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 011/013 | Batch 150/285 | Loss: 0.0055\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 011/013 | Batch 200/285 | Loss: 0.0010\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 011/013 | Batch 250/285 | Loss: 0.0040\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.26%\n",
            "Time elapsed: 2.23 min\n",
            "Epoch: 012/013 | Batch 000/285 | Loss: 0.0171\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 012/013 | Batch 050/285 | Loss: 0.0108\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 012/013 | Batch 100/285 | Loss: 0.0007\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 012/013 | Batch 150/285 | Loss: 0.0285\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 012/013 | Batch 200/285 | Loss: 0.0018\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 012/013 | Batch 250/285 | Loss: 0.0277\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.57%\n",
            "Time elapsed: 2.43 min\n",
            "Epoch: 013/013 | Batch 000/285 | Loss: 0.0233\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 013/013 | Batch 050/285 | Loss: 0.0034\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 013/013 | Batch 100/285 | Loss: 0.0012\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 013/013 | Batch 150/285 | Loss: 0.0245\n",
            "126 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 013/013 | Batch 200/285 | Loss: 0.0168\n",
            "127 out of 128\n",
            "-----------------------------------------------------------\n",
            "Epoch: 013/013 | Batch 250/285 | Loss: 0.0031\n",
            "128 out of 128\n",
            "-----------------------------------------------------------\n",
            "training accuracy: 99.54%\n",
            "Time elapsed: 2.63 min\n",
            "Total Training Time: 2.63 min\n",
            "Test accuracy: 96.04%\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "n_ep = params['epochs']\n",
        "\n",
        "for epoch in range(n_ep):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        answer, _ = batch_data.answer\n",
        "        target = batch_data.target.to(DEVICE)\n",
        "\n",
        "        odds = model(answer)\n",
        "        probs = torch.sigmoid(odds)\n",
        "        predicted_labels = (probs > 0.5).float()\n",
        "        loss = nn.BCEWithLogitsLoss()(odds, target.unsqueeze(1).to(odds.dtype))\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{n_ep:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Loss: {loss:.4f}')\n",
        "            \n",
        "            predicted_labels_np = predicted_labels.cpu().numpy()\n",
        "            target_np = target.cpu().numpy()\n",
        "\n",
        "            corrects = [1 for i, pred in enumerate(predicted_labels_np) if (target_np[i]==pred)]\n",
        "            correct_pred = sum(corrects)\n",
        "            num_examples = target_np.shape[0]\n",
        "\n",
        "            print(f\"{correct_pred} out of {num_examples}\")\n",
        "            print(\"-----------------------------------------------------------\")\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{get_accuracy(model, train_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {get_accuracy(model, test_loader, DEVICE):.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c4d97a",
      "metadata": {
        "id": "59c4d97a"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Function to test the model on any given text\n",
        "def predict_BotOrNot(model, text):\n",
        "    '''\n",
        "    Function to test a text-classifying model on any given text\n",
        "    '''\n",
        "    model.eval()\n",
        "    \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(text)]\n",
        "    indexed = [answer_field.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    \n",
        "    return (prediction[0][0].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da0d4f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9da0d4f2",
        "outputId": "2264e110-b1c0-46bb-885b-568159380d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability of being human: 0.00014332136197481304\n"
          ]
        }
      ],
      "source": [
        "# Example 1 - chatGPT decribing football\n",
        "pred = predict_BotOrNot(model, \"Football is a popular team sport played between two teams of eleven players, with the objective of scoring goals by kicking a ball into the opposing team's goalpost. The game is played on a rectangular field and involves passing, dribbling, and shooting skills. Football is a highly competitive and physically demanding sport, enjoyed by millions of fans worldwide.\")\n",
        "print(f'Probability of being human: {pred}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81rW-0DBVZrq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81rW-0DBVZrq",
        "outputId": "ef484117-f1a5-4cee-b803-52a08db6d3b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability of being human: 0.9733949899673462\n"
          ]
        }
      ],
      "source": [
        "# Example 2 - Human (me) decribing football\n",
        "pred = predict_BotOrNot(model, \"Football is probably the most popular sport in the world. There are eleven players in each team.\")\n",
        "print(f'Probability of being human: {pred}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KeuLxfeIPW_J",
      "metadata": {
        "id": "KeuLxfeIPW_J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
